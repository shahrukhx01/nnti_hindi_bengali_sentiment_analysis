{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VZXi_KGi0UR"
   },
   "source": [
    "# Task 1: Bengali Word Embeddings\n",
    "\n",
    "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48t-II1vkuau"
   },
   "source": [
    "## Imports\n",
    "\n",
    "This code block is reserved for your imports. \n",
    "\n",
    "You are free to use the following packages: \n",
    "\n",
    "(List of packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4kh6nh84-AOL"
   },
   "outputs": [],
   "source": [
    "############ loading all the necessary packages ############\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import matplotlib.pylab as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWmk3hVllEcU"
   },
   "source": [
    "# 1.1 Get the data\n",
    "\n",
    "\n",
    "If you are using Colab the first two lines will let you upload folders or files from your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "XtI7DJ-0-AOP",
    "outputId": "42a7b261-e2ff-404f-a836-bf7c761a8410"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "\n",
    "############ reading the the tsv file into data ############ \n",
    "data = pd.read_csv('data/bengali_hatespeech_subset.csv')\n",
    "\n",
    "data.head() # looking at the first 5 rows of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-XfPp6yCqAg",
    "outputId": "aec33817-4635-4f66-e8bc-75ee68ba2981"
   },
   "outputs": [],
   "source": [
    "#data = data.sample(100) # sampling 1000 data points from the loaded data\n",
    "data.shape # checking the shape of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-mSJ8nUlupB"
   },
   "source": [
    "## 1.2 Data preparation\n",
    "\n",
    "* Prepare the data by removing everything that does not contain information. \n",
    "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n",
    "Do this for the development section of the corpus for now.\n",
    "\n",
    "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEyZpk5GEaxM"
   },
   "outputs": [],
   "source": [
    "USERNAME_PATTERN = r'@([A-Za-z0-9_]+)' # specifying the pattern for identifying usernames\n",
    "PUNCTUATION_PATTERN = '\\'â€™|!@$%^&*()_+<>?:.,;-' # specifying the pattern for identifying punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH5L51dVJNAX"
   },
   "outputs": [],
   "source": [
    "stopwords_hindi_file = open('data/stopwords-bn.txt', 'r') # loading the file with stopwords for hindi dataset\n",
    "stopwords_hindi = [line.replace('\\n','') for line in stopwords_hindi_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHcNeyKi-AOQ"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "def remove_punctuations(text):\n",
    "  '''\n",
    "  this method removes the punctuations in a given text \n",
    "  using the PUNCTUATION_PATTERN specified.\n",
    "\n",
    "  Arguments: \n",
    "  text -- string containing the punctuations.\n",
    "\n",
    "  Return:\n",
    "  text -- string with no punctuations.\n",
    "  '''\n",
    "  return \"\".join([c for c in text if c not in PUNCTUATION_PATTERN])\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  '''\n",
    "  this method removes the stop words in a given text \n",
    "  using the stopwords_hindi specified.\n",
    "\n",
    "  Arguments: \n",
    "  text -- string containing the stop words.\n",
    "\n",
    "  Return:\n",
    "  text -- string with no stop words.\n",
    "  '''\n",
    "  return \" \".join([word for word in text.split() if word not in stopwords_hindi])\n",
    "\n",
    "\n",
    "\n",
    "def remove_usernames(text):  \n",
    "  '''\n",
    "  this method removes any usernames in a given text \n",
    "  using the USERNAME_PATTERN specified.\n",
    "\n",
    "  Arguments: \n",
    "  text -- string containing the usernames.\n",
    "\n",
    "  Return:\n",
    "  text -- string with no usernames.\n",
    "  '''\n",
    "  return re.sub(USERNAME_PATTERN, '', text)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFE0aAZmEwz1"
   },
   "outputs": [],
   "source": [
    "## normalizing text to lower case\n",
    "data['clean_text'] = data.sentence.apply(lambda text: text.lower())\n",
    "\n",
    "## removing usernames\n",
    "data['clean_text'] = data.clean_text.apply(remove_usernames)\n",
    "\n",
    "## removing punctuations\n",
    "data['clean_text'] = data.clean_text.apply(remove_punctuations)\n",
    "\n",
    "## removing stopwords\n",
    "data['clean_text'] = data.clean_text.apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je09nozLmmMm"
   },
   "source": [
    "## 1.3 Build the vocabulary \n",
    "\n",
    "The input to the first layer of word2vec is an one-hot encoding of the current word. The output of the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n",
    "\n",
    "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpoGmTKx-AOQ"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "############ forming a list V with all the unique values in our dataset ############\n",
    "V = list(data.clean_text.str.split(expand=True).stack().value_counts().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nXuzKlLUNTU"
   },
   "outputs": [],
   "source": [
    "## dictionary for mapping words to index\n",
    "word2index = {word:index for index,word in enumerate(V)}\n",
    "\n",
    "## dictionary for mapping index to words\n",
    "index2word = {index:word for index,word in enumerate(V)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiaVglVNoENY"
   },
   "source": [
    "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqPNw6IT-AOQ"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "def word_to_one_hot(word):\n",
    "  '''\n",
    "  this method converts every text word into a list of binary integers representing \n",
    "  the word numerically\n",
    "\n",
    "  Arguments: \n",
    "  word -- any string value of any variable length\n",
    "\n",
    "  Return:\n",
    "  list -- list of integers, numerical representation of the text word\n",
    "  '''\n",
    "  one_hot_encoding = [0]*len(V)\n",
    "  one_hot_encoding[word2index[word]] = 1.0\n",
    "  return one_hot_encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKD8zBlxVclh"
   },
   "source": [
    "## 1.4 Subsampling (0.5 points)\n",
    "\n",
    "The probability to keep a word in a context is given by:\n",
    "\n",
    "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
    "\n",
    "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
    "* Calculate word frequencies\n",
    "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj4sDOVMMr0b"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "word_frequencies = dict(data.clean_text.str.split(expand=True).stack().value_counts())\n",
    "total_frequency = sum(word_frequencies.values())\n",
    "\n",
    "def sampling_prob(word):\n",
    "  '''\n",
    "  this method takes in a text word and return back the probability of\n",
    "  keeping the word in the context using the specified subsampling formula\n",
    "\n",
    "  Arguments: \n",
    "  word -- any string value of any variable length\n",
    "\n",
    "  Return:\n",
    "  float64 numerical value -- probability score ranging from 0 to 1\n",
    "  '''\n",
    "  relative_frequency = word_frequencies[word]/total_frequency\n",
    "  return (np.sqrt(relative_frequency / .001) + 1 ) * (.001/relative_frequency)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxV1P90zplxu"
   },
   "source": [
    "# 1.5 Skip-Grams\n",
    "\n",
    "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
    "\n",
    "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
    "\n",
    "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8CCTpVy-AOR"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "def get_target_context(sentence, window_size):\n",
    "  '''\n",
    "  this method takes in a sentence and a window_size and returns back \n",
    "  the words of related context\n",
    "\n",
    "  Arguments: \n",
    "  sentence -- any string value of any variable length\n",
    "  window_size -- specifies the number of context related words to get\n",
    "\n",
    "  Return:\n",
    "  generator class object -- consisting of tuples, with each tuple having the current word\n",
    "                            and a list of context related words, number of context words\n",
    "                            as specified by the window size\n",
    "  '''\n",
    "\n",
    "  tokens = sentence.split()\n",
    "  for current_word_index, current_word in enumerate(tokens):\n",
    "    context = []\n",
    "    for context_word_index in range(current_word_index-window_size, current_word_index + window_size + 1):\n",
    "      ## check wthether context word index is within sequence and is not the current word itself.\n",
    "      if current_word_index != context_word_index and context_word_index <= len(tokens) -1 and context_word_index >=0:\n",
    "        \n",
    "        # increase sampling chances of infrequent words in context\n",
    "        if np.random.random() < sampling_prob(tokens[context_word_index]):\n",
    "          context.append(tokens[context_word_index])\n",
    "\n",
    "    yield (current_word, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3oq2DLU9ntY"
   },
   "source": [
    "## 1.5a Bengali Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS3TeHCZABZR"
   },
   "outputs": [],
   "source": [
    "class BengaliDataset:\n",
    "    ##### a custom dataset class for holding data for word2vec embeddings (using skip-gram) #####\n",
    "    def __init__(self, data, window_size, batch_size=32):\n",
    "      '''\n",
    "      This constructor is instantiated when an object of this class is created.\n",
    "      It sets the values for the data, window_size and batch_size for the object \n",
    "\n",
    "      Arguments: \n",
    "      data -- dataset that needs to be loaded\n",
    "      window_size -- integer value, specifies the number of context related words to get\n",
    "      batch_size -- integer value, number of observations per batch\n",
    "\n",
    "      '''\n",
    "      self.data = data\n",
    "      self.window_size = window_size\n",
    "      self.batch_size = batch_size\n",
    "\n",
    "    def load_data(self):\n",
    "        '''\n",
    "        this method loads the data by using the transform_data method \n",
    "        '''\n",
    "        for i in tqdm(range(len(self.data.clean_text.values))):\n",
    "          self.transform_data(i)\n",
    "        \n",
    "    def transform_data(self, index):\n",
    "      '''\n",
    "      this method transforms the data into inputs and labels  \n",
    "      '''\n",
    "      X, Y = [], []\n",
    "\n",
    "      ## get the text sequence from dataframe\n",
    "      sentence = self.data.clean_text.values[index]\n",
    "\n",
    "       ## fetch context words within the context window\n",
    "      for current_word, context in get_target_context(sentence, window_size=self.window_size):\n",
    "        current_word_onehot = word_to_one_hot(current_word)\n",
    "\n",
    "    ## iterate over context list and one hot encode them and align them with input\n",
    "        for context_word in context:\n",
    "          context_word_onehot = word2index[context_word]\n",
    "\n",
    "          X.append(current_word_onehot)\n",
    "          Y.append(context_word_onehot)\n",
    "\n",
    "      ## casting the lists to tensors, as forward pass expects float tensor and loss function expects long tensor\n",
    "      self.inputs, self.labels = torch.FloatTensor(X), torch.LongTensor(Y)\n",
    "    \n",
    "\n",
    "    def batchify(self):\n",
    "      '''\n",
    "      this method creates and returns back the batches of the dataset loaded \n",
    "      '''\n",
    "      index = 0\n",
    "      for index in range(0, len(self.inputs), self.batch_size):\n",
    "        yield (self.inputs[index:index+self.batch_size], self.labels[index:index+self.batch_size])\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEFgtkmuDjL"
   },
   "source": [
    "# 1.6 Hyperparameters\n",
    "\n",
    "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
    "\n",
    "* Embedding dimension\n",
    "* Window size\n",
    "\n",
    "Initialize them in a dictionary or as independent variables in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7xSKuFJcYoD"
   },
   "outputs": [],
   "source": [
    "## the paper used '10' as window_size and '300' embedding dimension, however, the best\n",
    "## window_size that worked for us was 2\n",
    "\n",
    "# Set hyperparameters\n",
    "window_size = 2\n",
    "embedding_size = 300\n",
    "input_size = len(V)\n",
    "batch_size = 8\n",
    "\n",
    "# More hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBWBAZHrEruy",
    "outputId": "f8bc0b00-cf91-4d8e-d46e-8eece064b8f7"
   },
   "outputs": [],
   "source": [
    "## instantiate HASOC dataset \n",
    "## window size changes over here\n",
    "print('loading and transforming data...')\n",
    "bengali_train_dataset = BengaliDataset(data, window_size, batch_size=batch_size)\n",
    "hasoc_dataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiM2zq-YunPx"
   },
   "source": [
    "# 1.7 Pytorch Module\n",
    "\n",
    "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
    "\n",
    "* Initialize the two weight matrices of word2vec as fields of the class.\n",
    "\n",
    "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
    "\n",
    "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9sGNytYhwxS"
   },
   "outputs": [],
   "source": [
    "# Create model \n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    '''\n",
    "    this constructor instantiates the nn.Linear modules \n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Linear(in_features=input_size, out_features=hidden_size, bias=False)\n",
    "    self.layer2 = nn.Linear(in_features=hidden_size, out_features=input_size, bias=False)\n",
    "    #self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "  def forward(self, one_hot):\n",
    "    '''\n",
    "    this method does the forward pass\n",
    "    '''\n",
    "    x = self.layer1(one_hot)\n",
    "    x = self.layer2(x)\n",
    "    # omitted logsoftmax since we use CrossEntropyLoss which has implicit NLL + Logsoftmax\n",
    "    # y = self.log_softmax(x)\n",
    "    return x\n",
    "\n",
    "def init_weights(m):\n",
    "    '''\n",
    "    this method initializes the weights using the normal distribution\n",
    "    or sets the pretrained weights of a model\n",
    "      \n",
    "    Argument: \n",
    "    m -- pretrained weights of the model \n",
    "    '''\n",
    "    if type(m) == nn.Linear:\n",
    "      nn.init.normal_(m.weight)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XefIDMMHv5zJ"
   },
   "source": [
    "# 1.8 Loss function and optimizer\n",
    "\n",
    "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9-Ino-e29w3"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "word2vec_model = Word2Vec(input_size=input_size, hidden_size=embedding_size) \n",
    "is_untrained = True # is_trained = 1 for the new training of the model\n",
    "\n",
    "if is_untrained: # checks the flag for the trained model \n",
    "  word2vec_model.apply(init_weights)\n",
    "else:\n",
    "  word2vec_model.load_state_dict(torch.load('/content/word2vec_ws{}.pth'.format(window_size)))\n",
    "\n",
    "word2vec_model = word2vec_model.to(device)\n",
    "word2vec_model.train(True)\n",
    "\n",
    "optimizer = torch.optim.Adam(word2vec_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss() # it has softmax + NLL combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckTfK78Ew8wI"
   },
   "source": [
    "# 1.9 Training the model\n",
    "\n",
    "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
    "\n",
    "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
    "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
    "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
    "\n",
    "You can play around with the number of epochs and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbMGD5L0mLDx",
    "outputId": "afefee54-23f1-411a-8ce5-333dc6011d24"
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "# Define train procedure\n",
    "def train():\n",
    "  '''\n",
    "  on calling this method the training loop will be executed for specified \n",
    "  number of epochs on the batches of dataset provided by the bachify method\n",
    "\n",
    "  Arguments: \n",
    "  takes no argument\n",
    "\n",
    "  Return:\n",
    "  gives no return \n",
    "\n",
    "  '''\n",
    "  min_loss = 1e3\n",
    "  threshold = -1\n",
    "  print(\"Training started for lr {}\".format(learning_rate))\n",
    "   \n",
    "  for epoch in range(epochs):\n",
    "    loss_val = []\n",
    "    \n",
    "    for (X,y) in hasoc_dataset.batchify():\n",
    "        X= X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        ## forward pass\n",
    "        output = word2vec_model(X)\n",
    "        loss = criterion(output, y)\n",
    "        ## backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val.append(loss.item())\n",
    "    \n",
    "    if np.mean(loss_val) < min_loss:\n",
    "      min_loss = np.mean(loss_val)\n",
    "      print('new model saved with epoch loss {}'.format(min_loss))\n",
    "      torch.save(word2vec_model.state_dict(), '/content/models/word2vec_ws{}.pth'.format(window_size))\n",
    "\n",
    "    if (epoch+1) % 1 == 0:\n",
    "      print (f'Epoch [{epoch+1}/{epochs}], Loss: {np.mean(loss_val):.4f}')\n",
    "      loss_list.append(np.mean(loss_val))\n",
    "\n",
    "train()\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQkaYstyj0Q"
   },
   "source": [
    "# 1.10 Train on the full dataset\n",
    "\n",
    "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
    "\n",
    "* Then, retrain your model on the complete dataset.\n",
    "\n",
    "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwTdN52FNdJt",
    "outputId": "27a02d7c-ae76-48c9-840a-2aba1e8c050f"
   },
   "outputs": [],
   "source": [
    "word2vec_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6MH2up2LCIf",
    "outputId": "bde123cb-642a-44ae-89ac-7816934971db"
   },
   "outputs": [],
   "source": [
    "## inferencing outputs for the word: modi\n",
    "predictions = word2vec_model(torch.unsqueeze(torch.tensor(word_to_one_hot('modi')), 0).cuda())\n",
    "\n",
    "\n",
    "## sampling the top k neighbors for the input word\n",
    "for i in torch.topk(predictions, 15)[1][0]:\n",
    "  print(index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUl-e1GGNk7t"
   },
   "outputs": [],
   "source": [
    "## we are only saving the embedding weights as we are able to reproduce vocabulary items in our scripts\n",
    "\n",
    "pickle_file = 'bengali_embedding_weights_all_window_2.pickle'\n",
    "embedding_weights = word2vec_model.layer1.weight.data.detach().cpu().numpy()\n",
    "\n",
    "with open(\"bengali_embedding_weights_all_window_2.pickle\", \"wb\") as f:\n",
    "  pickle.dump(embedding_weights, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "3j00UOQIh4Dl",
    "outputId": "9606fed1-2809-48e2-8c06-9cae0b6b2cce"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(pickle_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task1_Word_Embeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
